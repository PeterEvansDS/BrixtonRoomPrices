{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8e3a38",
   "metadata": {},
   "source": [
    "# Brixton Room Prices\n",
    "\n",
    "This notebook is used to implement a regression model, designed to predict the going rate of rooms in shared houses in the London district of Brixton. The data is scraped from [SpareRoom](https://spareroom.co.uk).\n",
    "\n",
    "The idea of the project was to write our code as a testable and maintainable Python package with entry-points to tune, train and test our model so it can easily be integrated into a CI/CD flow. This was attempted after viewing a [tutorial](https://morioh.com/p/d9fffafd5f92) on maintainable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf1839",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a96af",
   "metadata": {},
   "source": [
    "As the project uses custom built classes and functions, written in a text editor / IDE for convenience, the `autoreload` method below ensures that the notebook does not have to be restarted every time this code is altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f464c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041e554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e02bc",
   "metadata": {},
   "source": [
    "## Scraping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea91ef",
   "metadata": {},
   "source": [
    "The `SpareRoomScraper` object takes an ID on init. This is found in the url of any search made through SpareRoom, it simply has to be copied and pasted - this could be any location. The class uses the HTMLRequests library to scrape the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ce078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape import SpareRoomScraper\n",
    "\n",
    "scraper = SpareRoomScraper(search_id = 1097663249)\n",
    "df = scraper.get_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_feather('./data/brixton.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148c9ac",
   "metadata": {},
   "source": [
    "## Pipeline and ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5f0e9",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "It has been attempted to use sklearn's preprocessing framework to complete all of the preprocessing. The idea being, inspired by the aforementioned [tutorial](https://morioh.com/p/d9fffafd5f92), that the process of preparing the data for modelling can be undertaken using a combination of `Pipeline` and `ColumnTransformer` objects enabling the optimisation of their parameters through `GridSearchCV` just as is the case for the regression models.\n",
    "\n",
    "However, `Pipeline` objects are limited in that they can only transform the X matrix - any transformation completed on the target must be completed externally beforehand. In this case, this means the removal of any listings that are whole properties rather than rooms, and the removal or imputation of any missing values within the dataframe.\n",
    "\n",
    "Thus, while the parameters of many of the preprocessing classes and the regression algorithm can be altered through `GridSearchCV`, the imputation method must be chosen beforehand.\n",
    "\n",
    "There are many preprocessing steps that are completed within the pipeline however, using the classes in `preprocessing.py`. These include:\n",
    "\n",
    "  - The extraction of separation of distinct pieces of information within some features into their own separate features. E.g. the original `distance_to_station` feature is split into the distance and the mode of transport used to get there (walk or bus).\n",
    "  - Transformation of some features into a more useful form - `availability` is transformed into `available_in` which is simply and integer of the number of days between the current day and the room's availability date.\n",
    "  - Encoding of Yes/No features in a binary format, and other categorical features in a one-hot format where appropriate\n",
    "\n",
    "In addition to `preprocessing.py`, the project also uses `encoders.py` to encode the features in a format suitable for modelling.\n",
    "\n",
    "#### Model building\n",
    "\n",
    "The preprocessing steps are combined with a regression algorithm using a `Pipeline` object. We use a dummy estimator within the pipe so that the algorithm can be set with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "126a106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 902.59003035,  853.43336107,  915.55769054,  971.71401698,\n",
       "       1063.98920047])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing import prep_for_model\n",
    "from model import build_model\n",
    "from sklearn import linear_model\n",
    "\n",
    "df = pd.read_feather('./data/brixton.feather')\n",
    "X_train, X_test, y_train, y_test = prep_for_model(df)\n",
    "\n",
    "model = build_model()\n",
    "model.set_params(**{'model':linear_model.LinearRegression()})\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_train)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5977f3",
   "metadata": {},
   "source": [
    "## Tuning and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a89807",
   "metadata": {},
   "source": [
    "Now that we have a working pipeline, many different algorithms and parameters can be easily tested. The inputted parameters are listed in `config.py`.\n",
    "\n",
    "The algorithms tested are Ridge and Lasso linear models along with Decision Tree and Random Forest regressors, scored with the coefficient of determination, R<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af8c7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'model': RandomForestRegressor(max_features=5), 'model__max_features': 5, 'model__n_estimators': 100}\n",
      "Best score: 0.23953676341307698\n"
     ]
    }
   ],
   "source": [
    "from model import tune_model\n",
    "df = pd.read_feather('./data/brixton.feather')\n",
    "tune_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9cefc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the test set: -0.24240644689106028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from model import train_model\n",
    "from model import test_model\n",
    "\n",
    "params = {\n",
    "    'model': RandomForestRegressor(max_features = 5, n_estimators = 200),\n",
    "    'model__max_features': 5,\n",
    "    'model__n_estimators': 200\n",
    "}\n",
    "\n",
    "train_model(df, './models/model.joblib', params)\n",
    "test_model(df, './models/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae77e00",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d93cc",
   "metadata": {},
   "source": [
    "It's clear to see that the performance is pretty poor, even with the best performing algorithm. It's likely that this is simply just a result of the data being collected not being the main factors in price. For example, there's no listing of the actual room size on SpareRoom listings - a very important factor in how much the rent will be! \n",
    "\n",
    "But, adding in additional features is relatively straight-forward given the modular nature of the web scraper and the model testing. So over time more improvements can be made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
